#!/usr/bin/env python3
"""
AI Hub Documentation Ingestion Script (Incremental)
Ingests markdown files from homelab-migration repo into Qdrant
Only processes new/changed files to save OpenAI API costs
"""

import os
import sys
import json
import hashlib
import subprocess
from pathlib import Path
from typing import List, Dict, Set
from datetime import datetime

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from openai import OpenAI

# Configuration from environment
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
QDRANT_URL = os.getenv('QDRANT_URL', '{{ qdrant_url }}')
QDRANT_COLLECTION = os.getenv('QDRANT_COLLECTION', '{{ qdrant_collection }}')
DOCS_PATH = os.getenv('DOCS_PATH', '{{ docs_repo_path }}')
EMBEDDING_MODEL = '{{ embedding_model }}'
EMBEDDING_DIMENSIONS = {{ embedding_dimensions }}

# State tracking
STATE_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'ingestion_state.json')

# Chunk settings
CHUNK_SIZE = 1000  # characters
CHUNK_OVERLAP = 200  # characters


def load_state() -> Dict:
    """Load previous ingestion state (file hashes)"""
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"Warning: Could not load state file: {e}")
    return {'file_hashes': {}, 'last_run': None}


def save_state(state: Dict):
    """Save current ingestion state"""
    state['last_run'] = datetime.now().isoformat()
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f, indent=2)


def get_file_hash(file_path: str) -> str:
    """Calculate SHA256 hash of file content"""
    with open(file_path, 'rb') as f:
        return hashlib.sha256(f.read()).hexdigest()


def chunk_text(text: str, file_path: str) -> List[Dict]:
    """Split text into overlapping chunks"""
    chunks = []
    start = 0
    chunk_num = 0

    while start < len(text):
        end = start + CHUNK_SIZE
        chunk_text = text[start:end]

        # Don't create tiny chunks at the end
        if len(chunk_text) < 100 and chunks:
            chunks[-1]['text'] += ' ' + chunk_text
            break

        chunks.append({
            'text': chunk_text,
            'file_path': file_path,
            'chunk_num': chunk_num,
            'chunk_id': f"{file_path}:chunk{chunk_num}"
        })

        chunk_num += 1
        start = end - CHUNK_OVERLAP

    return chunks


def find_markdown_files(root_path: str) -> List[str]:
    """Find all markdown files in directory"""
    md_files = []
    for path in Path(root_path).rglob('*.md'):
        # Skip node_modules and hidden directories
        if 'node_modules' in str(path) or '/.git' in str(path):
            continue
        md_files.append(str(path))
    return md_files


def generate_embedding(text: str, client: OpenAI) -> List[float]:
    """Generate embedding for text using OpenAI"""
    response = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=text
    )
    return response.data[0].embedding


def get_chunk_point_id(chunk_id: str) -> int:
    """Generate consistent point ID from chunk_id"""
    return int(hashlib.md5(chunk_id.encode()).hexdigest()[:16], 16) % (2**63)


def delete_file_chunks(qdrant: QdrantClient, file_path: str):
    """Delete all chunks for a specific file from Qdrant"""
    try:
        # Delete points where file_path matches
        qdrant.delete(
            collection_name=QDRANT_COLLECTION,
            points_selector=Filter(
                must=[
                    FieldCondition(
                        key="file_path",
                        match=MatchValue(value=file_path)
                    )
                ]
            )
        )
        print(f"  → Deleted chunks for: {file_path}")
    except Exception as e:
        print(f"  → Warning: Could not delete chunks for {file_path}: {e}")


def git_pull(repo_path: str) -> bool:
    """Pull latest changes from git"""
    try:
        result = subprocess.run(
            ['git', 'pull', '--ff-only'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=60
        )
        if result.returncode == 0:
            if 'Already up to date' in result.stdout:
                print("Repository already up to date")
            else:
                print(f"Git pull successful:\n{result.stdout.strip()}")
            return True
        else:
            print(f"Git pull failed: {result.stderr}")
            return False
    except Exception as e:
        print(f"Git pull error: {e}")
        return False


def main():
    print("=" * 60)
    print("AI Hub Documentation Ingestion (Incremental)")
    print(f"Started: {datetime.now().isoformat()}")
    print("=" * 60)

    # Load previous state
    state = load_state()
    old_hashes = state.get('file_hashes', {})
    if state.get('last_run'):
        print(f"Last run: {state['last_run']}")

    # Pull latest from git
    print(f"\nPulling latest changes from {DOCS_PATH}...")
    if not git_pull(DOCS_PATH):
        print("Warning: Git pull failed, continuing with existing files")

    # Initialize clients
    print(f"\nConnecting to Qdrant at {QDRANT_URL}...")
    qdrant = QdrantClient(url=QDRANT_URL)

    print("Connecting to OpenAI...")
    openai_client = OpenAI(api_key=OPENAI_API_KEY)

    # Find markdown files
    print(f"\nSearching for markdown files in {DOCS_PATH}...")
    md_files = find_markdown_files(DOCS_PATH)
    print(f"Found {len(md_files)} markdown files")

    if not md_files:
        print("No markdown files found!")
        return 1

    # Calculate current file hashes and determine changes
    current_hashes = {}
    new_files = []
    changed_files = []
    unchanged_files = []

    current_rel_paths: Set[str] = set()

    for file_path in md_files:
        rel_path = os.path.relpath(file_path, DOCS_PATH)
        current_rel_paths.add(rel_path)
        file_hash = get_file_hash(file_path)
        current_hashes[rel_path] = file_hash

        if rel_path not in old_hashes:
            new_files.append((file_path, rel_path))
        elif old_hashes[rel_path] != file_hash:
            changed_files.append((file_path, rel_path))
        else:
            unchanged_files.append(rel_path)

    # Find deleted files
    deleted_files = set(old_hashes.keys()) - current_rel_paths

    # Summary
    print(f"\n--- Change Summary ---")
    print(f"  New files:       {len(new_files)}")
    print(f"  Changed files:   {len(changed_files)}")
    print(f"  Unchanged files: {len(unchanged_files)}")
    print(f"  Deleted files:   {len(deleted_files)}")

    # Handle deleted files
    if deleted_files:
        print(f"\nRemoving deleted files from Qdrant...")
        for rel_path in deleted_files:
            delete_file_chunks(qdrant, rel_path)

    # Process new and changed files
    files_to_process = new_files + changed_files

    if not files_to_process:
        print("\nNo new or changed files to process!")
        state['file_hashes'] = current_hashes
        save_state(state)
        print("\n" + "=" * 60)
        print("Ingestion completed - no changes needed")
        print("=" * 60)
        return 0

    print(f"\nProcessing {len(files_to_process)} files...")

    all_chunks = []
    processed_files = []

    for file_path, rel_path in files_to_process:
        status = "NEW" if (file_path, rel_path) in new_files else "CHANGED"
        print(f"[{status}] Processing: {rel_path}")

        # For changed files, delete old chunks first
        if status == "CHANGED":
            delete_file_chunks(qdrant, rel_path)

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            chunks = chunk_text(content, rel_path)
            all_chunks.extend(chunks)
            processed_files.append(rel_path)
            print(f"  → Created {len(chunks)} chunks")

        except Exception as e:
            print(f"  → Error: {e}")
            continue

    if not all_chunks:
        print("\nNo chunks to process!")
        state['file_hashes'] = current_hashes
        save_state(state)
        return 0

    print(f"\nTotal chunks to ingest: {len(all_chunks)}")

    # Generate embeddings and upload to Qdrant
    print("\nGenerating embeddings and uploading to Qdrant...")
    points = []
    successful_chunks = 0

    for i, chunk in enumerate(all_chunks, 1):
        if i % 10 == 0 or i == len(all_chunks):
            print(f"  Processing chunk {i}/{len(all_chunks)}...")

        try:
            # Generate embedding
            embedding = generate_embedding(chunk['text'], openai_client)

            # Create point ID from chunk_id hash
            point_id = get_chunk_point_id(chunk['chunk_id'])

            # Create point
            point = PointStruct(
                id=point_id,
                vector=embedding,
                payload={
                    'text': chunk['text'],
                    'file_path': chunk['file_path'],
                    'chunk_num': chunk['chunk_num'],
                    'chunk_id': chunk['chunk_id'],
                    'source': 'homelab-docs',
                    'ingested_at': datetime.now().isoformat()
                }
            )
            points.append(point)
            successful_chunks += 1

            # Upload in batches of 100
            if len(points) >= 100:
                qdrant.upsert(
                    collection_name=QDRANT_COLLECTION,
                    points=points
                )
                points = []

        except Exception as e:
            print(f"  → Error processing chunk {i}: {e}")
            continue

    # Upload remaining points
    if points:
        qdrant.upsert(
            collection_name=QDRANT_COLLECTION,
            points=points
        )

    # Save state with updated hashes
    state['file_hashes'] = current_hashes
    save_state(state)

    print("\n" + "=" * 60)
    print("Ingestion completed successfully!")
    print(f"  Files processed: {len(processed_files)}")
    print(f"  Chunks ingested: {successful_chunks}")
    print(f"  Files deleted:   {len(deleted_files)}")
    print(f"  Files unchanged: {len(unchanged_files)} (skipped)")
    print("=" * 60)

    return 0


if __name__ == '__main__':
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nIngestion cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nFatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
